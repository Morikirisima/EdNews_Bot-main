import requests
from bs4 import BeautifulSoup
from datetime import datetime
from database.models.models import Post  # ‚Üê –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç
from utils.filters import is_advertisement_content, needs_manual_review
import config


class SimpleParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    async def parse_new_posts(self) -> list:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            print(f"üîç –ü–∞—Ä—Å–∏–º {config.PARSER_URL}")
            response = self.session.get(config.PARSER_URL)
            soup = BeautifulSoup(response.content, 'html.parser')

            posts_data = self._extract_posts(soup)
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(posts_data)} –ø–æ—Å—Ç–æ–≤")
            return posts_data

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return []

    def _extract_posts(self, soup) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å—Ç—ã –∏–∑ HTML"""
        posts = []

        articles = soup.find_all('article', limit=10)
        if not articles:
            articles = soup.find_all('div', class_='ItemOfListStandard_item__eAfc4', limit=10)

        for article in articles:
            post_data = self._parse_article(article)
            if post_data and post_data.get('title') and post_data['title'] != "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞":
                posts.append(post_data)

        return posts

    def _parse_article(self, article) -> dict:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é"""
        try:
            title_elem = (article.find('h3') or
                          article.find('h2') or
                          article.find('span', class_='ItemOfListStandard_title__Ajjlf') or
                          article.find('a').find('span') if article.find('a') else None)

            title = title_elem.get_text().strip() if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

            link_elem = article.find('a')
            url = link_elem.get('href') if link_elem else ""
            if url and url.startswith('/'):
                url = 'https://rg.ru' + url

            content_elem = (article.find('p') or
                            article.find('div', class_='preview') or
                            article.find('div', class_='ItemOfListStandard_announce__cOc_i'))
            content = content_elem.get_text().strip() if content_elem else ""

            if not content and url:
                content = self._get_full_content(url)

            return {
                'title': title,
                'content': content,
                'source_url': url,
                'published_at': datetime.now()
            }

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
            return {
                'title': "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞",
                'content': "",
                'source_url': "",
                'published_at': datetime.now()
            }

    def _get_full_content(self, url: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
        try:
            if not url:
                return ""

            print(f"    üìñ –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {url}")
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            content_blocks = [
                soup.find('div', class_='PageArticleContent_lead__l9TkG'),
                soup.find('div', class_='PageContentCommonStyling_text__CKOzO'),
                soup.find('div', class_='article-content'),
                soup.find('div', class_='article-body'),
                soup.find('article')
            ]

            for block in content_blocks:
                if block:
                    paragraphs = block.find_all('p')
                    text = '\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
                    if text:
                        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        return text

            print("    ‚ùå –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return ""

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
            return ""


async def process_parsed_posts(posts_data: list) -> list:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã: —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å—ã"""
    processed_posts = []

    for post_data in posts_data:
        if not post_data:
            continue

        full_text = f"{post_data['title']} {post_data['content']}"

        if is_advertisement_content(full_text):
            status = "rejected"
            print(f"üö´ –û—Ç–∫–ª–æ–Ω–µ–Ω (—Ä–µ–∫–ª–∞–º–∞): {post_data['title']}")

        elif needs_manual_review(full_text):
            status = "draft"
            print(f"üìù –ù–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é: {post_data['title']}")

        else:
            status = "parsed"
            print(f"‚úÖ –í –æ—á–µ—Ä–µ–¥—å: {post_data['title']}")

        processed_posts.append({
            **post_data,
            'status': status,
            'needs_review': status == "draft"
        })

    return processed_posts


async def save_posts_to_db(posts: list, db_session):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Å—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    from sqlalchemy import select

    saved_count = 0

    for post_data in posts:
        if not post_data:
            continue

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º Post (—Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
            existing_post = await db_session.execute(
                select(Post).where(Post.source_url == post_data['source_url'])  # ‚Üê Post
            )
            if existing_post.scalar():
                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: {post_data['title']}")
                continue

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø–æ—Å—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º Post (—Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
            new_post = Post(  # ‚Üê Post
                title=post_data['title'],
                content=post_data['content'],
                source_url=post_data['source_url'],
                status=post_data['status']
            )

            db_session.add(new_post)
            saved_count += 1
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω: {post_data['title']}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å—Ç–∞: {e}")
            continue

    await db_session.commit()
    print(f"üíæ –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")
    return saved_count